{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from base64 import b64encode\n",
    "import urllib.request, hashlib, requests, multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.sec.gov/files/EDGAR_LogFileData_thru_Jun2017.html\"\n",
    "file = urllib.request.urlopen(url)\n",
    "lst = list()\n",
    "for line in file:\n",
    "\tdecoded_line = line.decode(\"utf-8\").strip()\n",
    "\tlst.append(decoded_line)\n",
    "lst = lst[8:-2]\n",
    "lst.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a TSV file of URLS to transfer job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_url(url, chunk_size=128, save_path='./tmp'):\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(save_path, 'wb') as fd:\n",
    "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "            fd.write(chunk)\n",
    "    return save_path\n",
    "\n",
    "def contentLength(link):\n",
    "    site = urllib.request.urlopen('http://' + link)\n",
    "    meta = site.info()\n",
    "    return meta['Content-Length']\n",
    "\n",
    "def checksum(link, hash_factory=hashlib.md5, chunk_num_blocks=128):\n",
    "    url = 'http://' + link\n",
    "    filename = download_url(url)\n",
    "    h = hash_factory()\n",
    "    with open(filename, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_num_blocks*h.block_size), b''): \n",
    "            h.update(chunk)\n",
    "    return b64encode(h.digest())\n",
    "\n",
    "def checksum_md5(link):\n",
    "    url = 'http://' + link\n",
    "    filename = download_url(url)\n",
    "    md5 = hashlib.md5()\n",
    "    with open(filename,'rb') as f: \n",
    "        for chunk in iter(lambda: f.read(8192), b''): \n",
    "            md5.update(chunk)\n",
    "    return b64encode(md5.digest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for idx, lnk in enumerate(lst[:]):\n",
    "    link, size, index = ( \"TsvHttpData-1.0\", None, None) if idx == 0 else ('http://' + lnk, contentLength(lnk), checksum(lnk))\n",
    "    df = df.append([[link, size, index]], ignore_index=True)\n",
    "\n",
    "df.to_csv('./file-name.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
